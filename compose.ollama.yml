version: "3.8"

services:
  # Ollama service
  ollama:
    image: ollama/ollama
    pull_policy: always
    container_name: ollama-service
    hostname: ollama
    platform: linux/amd64
    env_file:
      - .env
      - docker/ollama/ollama.env
    tty: false
    ports:
      - "22434:11434"
    volumes:
      - ${HOST_MODELS_DIR:?}:/root/.ollama/models
      - pip_cache_vol:/root/.cache/pip
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]
    networks:
      - opendevin-net
    tmpfs:
      - /run
      - /var/run
      - /tmp
      - /var/tmp
