# ü§ñ Backends LLM

:::note
Cette section est destin√©e aux utilisateurs qui souhaitent connecter OpenHands √† diff√©rents LLMs.
:::

OpenHands peut se connecter √† n'importe quel LLM pris en charge par LiteLLM. Cependant, il n√©cessite un mod√®le puissant pour fonctionner.

## Recommandations de mod√®les

Sur la base de nos √©valuations des mod√®les de langage pour les t√¢ches de programmation (utilisant le jeu de donn√©es SWE-bench), nous pouvons fournir quelques
recommandations pour la s√©lection de mod√®les. Nos derniers r√©sultats d'√©valuation peuvent √™tre consult√©s dans [ce tableur](https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0).

Sur la base de ces r√©sultats et des retours de la communaut√©, les mod√®les suivants ont √©t√© v√©rifi√©s comme fonctionnant raisonnablement bien avec OpenHands :

- [anthropic/claude-sonnet-4-20250514](https://www.anthropic.com/api) (recommand√©)
- [gemini/gemini-2.5-pro](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)
- [deepseek/deepseek-chat](https://api-docs.deepseek.com/)
- [openai/o3-mini](https://openai.com/index/openai-o3-mini/)
- [openai/o3](https://openai.com/index/introducing-o3-and-o4-mini/)
- [openai/o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)
- [all-hands/openhands-lm-32b-v0.1](https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model) -- disponible via [OpenRouter](https://openrouter.ai/all-hands/openhands-lm-32b-v0.1)


:::warning
OpenHands enverra de nombreuses requ√™tes au LLM que vous configurez. La plupart de ces LLMs ont un co√ªt, alors assurez-vous de d√©finir des limites de d√©penses et de surveiller l'utilisation.
:::

Si vous avez r√©ussi √† ex√©cuter OpenHands avec des LLMs sp√©cifiques qui ne figurent pas dans la liste, veuillez les ajouter √† la liste v√©rifi√©e. Nous
vous encourageons √©galement √† ouvrir une PR pour partager votre processus de configuration afin d'aider d'autres utilisateurs du m√™me fournisseur et LLM !

Pour une liste compl√®te des fournisseurs et mod√®les disponibles, veuillez consulter la
[documentation litellm](https://docs.litellm.ai/docs/providers).

:::note
La plupart des mod√®les locaux et open source actuels ne sont pas aussi puissants. Lorsque vous utilisez de tels mod√®les, vous pourriez constater de longs
temps d'attente entre les messages, des r√©ponses m√©diocres ou des erreurs concernant un JSON mal form√©. OpenHands ne peut √™tre que aussi puissant que les
mod√®les qui l'alimentent. Cependant, si vous en trouvez qui fonctionnent, veuillez les ajouter √† la liste v√©rifi√©e ci-dessus.
:::

## Configuration LLM

Les √©l√©ments suivants peuvent √™tre d√©finis dans l'interface utilisateur d'OpenHands via les Param√®tres :

- `Fournisseur LLM`
- `Mod√®le LLM`
- `Cl√© API`
- `URL de base` (via les param√®tres `Avanc√©s`)

Il existe certains param√®tres qui peuvent √™tre n√©cessaires pour certains LLMs/fournisseurs qui ne peuvent pas √™tre d√©finis via l'interface utilisateur. Au lieu de cela, ils
peuvent √™tre d√©finis via des variables d'environnement transmises √† la commande docker run lors du d√©marrage de l'application
en utilisant `-e` :

- `LLM_API_VERSION`
- `LLM_EMBEDDING_MODEL`
- `LLM_EMBEDDING_DEPLOYMENT_NAME`
- `LLM_DROP_PARAMS`
- `LLM_DISABLE_VISION`
- `LLM_CACHING_PROMPT`

Nous avons quelques guides pour ex√©cuter OpenHands avec des fournisseurs de mod√®les sp√©cifiques :

- [Azure](llms/azure-llms)
- [Google](llms/google-llms)
- [Groq](llms/groq)
- [LLMs locaux avec SGLang ou vLLM](llms/../local-llms.md)
- [Proxy LiteLLM](llms/litellm-proxy)
- [OpenAI](llms/openai-llms)
- [OpenRouter](llms/openrouter)

### Nouvelles tentatives d'API et limites de taux

Les fournisseurs de LLM ont g√©n√©ralement des limites de taux, parfois tr√®s basses, et peuvent n√©cessiter des nouvelles tentatives. OpenHands r√©essaiera automatiquement
les requ√™tes s'il re√ßoit une erreur de limite de taux (code d'erreur 429).

Vous pouvez personnaliser ces options selon vos besoins pour le fournisseur que vous utilisez. Consultez leur documentation et d√©finissez les
variables d'environnement suivantes pour contr√¥ler le nombre de nouvelles tentatives et le temps entre les tentatives :

- `LLM_NUM_RETRIES` (Par d√©faut 4 fois)
- `LLM_RETRY_MIN_WAIT` (Par d√©faut 5 secondes)
- `LLM_RETRY_MAX_WAIT` (Par d√©faut 30 secondes)
- `LLM_RETRY_MULTIPLIER` (Par d√©faut 2)

Si vous ex√©cutez OpenHands en mode d√©veloppement, vous pouvez √©galement d√©finir ces options dans le fichier `config.toml` :

```toml
[llm]
num_retries = 4
retry_min_wait = 5
retry_max_wait = 30
retry_multiplier = 2
```
