

# ü§ñ Backends LLM

OpenHands peut se connecter √† n'importe quel LLM support√© par LiteLLM. Cependant, il n√©cessite un mod√®le puissant pour fonctionner.

## Recommandations de mod√®les

Sur la base de nos √©valuations des mod√®les de langage pour les t√¢ches de codage (en utilisant le jeu de donn√©es SWE-bench), nous pouvons fournir quelques recommandations pour la s√©lection des mod√®les. Certaines analyses peuvent √™tre trouv√©es dans [cet article de blog comparant les LLM](https://www.all-hands.dev/blog/evaluation-of-llms-as-coding-agents-on-swe-bench-at-30x-speed) et [cet article de blog avec des r√©sultats plus r√©cents](https://www.all-hands.dev/blog/openhands-codeact-21-an-open-state-of-the-art-software-development-agent).

Lors du choix d'un mod√®le, consid√©rez √† la fois la qualit√© des sorties et les co√ªts associ√©s. Voici un r√©sum√© des r√©sultats :

- Claude 3.5 Sonnet est le meilleur de loin, atteignant un taux de r√©solution de 53% sur SWE-Bench Verified avec l'agent par d√©faut dans OpenHands.
- GPT-4o est √† la tra√Æne, et o1-mini a en fait obtenu des performances l√©g√®rement inf√©rieures √† celles de GPT-4o. Nous avons analys√© les r√©sultats un peu, et bri√®vement, il semblait que o1 "r√©fl√©chissait trop" parfois, effectuant des t√¢ches de configuration d'environnement suppl√©mentaires alors qu'il aurait pu simplement aller de l'avant et terminer la t√¢che.
- Enfin, les mod√®les ouverts les plus puissants √©taient Llama 3.1 405 B et deepseek-v2.5, et ils ont obtenu des performances raisonnables, surpassant m√™me certains des mod√®les ferm√©s.

Veuillez vous r√©f√©rer √† [l'article complet](https://www.all-hands.dev/blog/evaluation-of-llms-as-coding-agents-on-swe-bench-at-30x-speed) pour plus de d√©tails.

Sur la base de ces r√©sultats et des commentaires de la communaut√©, il a √©t√© v√©rifi√© que les mod√®les suivants fonctionnent raisonnablement bien avec OpenHands :

- claude-3-5-sonnet (recommand√©)
- gpt-4 / gpt-4o
- llama-3.1-405b
- deepseek-v2.5

:::warning
OpenHands enverra de nombreuses invites au LLM que vous configurez. La plupart de ces LLM sont payants, alors assurez-vous de d√©finir des limites de d√©penses et de surveiller l'utilisation.
:::

Si vous avez r√©ussi √† ex√©cuter OpenHands avec des LLM sp√©cifiques qui ne figurent pas dans la liste, veuillez les ajouter √† la liste v√©rifi√©e. Nous vous encourageons √©galement √† ouvrir une PR pour partager votre processus de configuration afin d'aider les autres utilisant le m√™me fournisseur et LLM !

Pour une liste compl√®te des fournisseurs et des mod√®les disponibles, veuillez consulter la [documentation litellm](https://docs.litellm.ai/docs/providers).

:::note
La plupart des mod√®les locaux et open source actuels ne sont pas aussi puissants. Lors de l'utilisation de tels mod√®les, vous pouvez constater de longs temps d'attente entre les messages, des r√©ponses m√©diocres ou des erreurs concernant du JSON mal form√©. OpenHands ne peut √™tre aussi puissant que les mod√®les qui le pilotent. Cependant, si vous en trouvez qui fonctionnent, veuillez les ajouter √† la liste v√©rifi√©e ci-dessus.
:::

## Configuration LLM

Les √©l√©ments suivants peuvent √™tre d√©finis dans l'interface utilisateur d'OpenHands via les param√®tres :

- `Fournisseur LLM`
- `Mod√®le LLM`
- `Cl√© API`
- `URL de base` (via `Param√®tres avanc√©s`)

Il existe certains param√®tres qui peuvent √™tre n√©cessaires pour certains LLM/fournisseurs et qui ne peuvent pas √™tre d√©finis via l'interface utilisateur. Au lieu de cela, ils peuvent √™tre d√©finis via des variables d'environnement pass√©es √† la [commande docker run](/modules/usage/installation#start-the-app) en utilisant `-e` :

- `LLM_API_VERSION`
- `LLM_EMBEDDING_MODEL`
- `LLM_EMBEDDING_DEPLOYMENT_NAME`
- `LLM_DROP_PARAMS`
- `LLM_DISABLE_VISION`
- `LLM_CACHING_PROMPT`

Nous avons quelques guides pour ex√©cuter OpenHands avec des fournisseurs de mod√®les sp√©cifiques :

- [Azure](llms/azure-llms)
- [Google](llms/google-llms)
- [Groq](llms/groq)
- [LiteLLM Proxy](llms/litellm-proxy)
- [OpenAI](llms/openai-llms)
- [OpenRouter](llms/openrouter)

### Nouvelles tentatives d'API et limites de d√©bit

Les fournisseurs de LLM ont g√©n√©ralement des limites de d√©bit, parfois tr√®s basses, et peuvent n√©cessiter de nouvelles tentatives. OpenHands r√©essaiera automatiquement les requ√™tes s'il re√ßoit une erreur de limite de d√©bit (code d'erreur 429), une erreur de connexion API ou d'autres erreurs transitoires.

Vous pouvez personnaliser ces options selon vos besoins pour le fournisseur que vous utilisez. Consultez leur documentation et d√©finissez les variables d'environnement suivantes pour contr√¥ler le nombre de nouvelles tentatives et le temps entre les tentatives :

- `LLM_NUM_RETRIES` (Par d√©faut 8)
- `LLM_RETRY_MIN_WAIT` (Par d√©faut 15 secondes)
- `LLM_RETRY_MAX_WAIT` (Par d√©faut 120 secondes)
- `LLM_RETRY_MULTIPLIER` (Par d√©faut 2)

Si vous ex√©cutez OpenHands en mode d√©veloppement, vous pouvez √©galement d√©finir ces options dans le fichier `config.toml` :

```toml
[llm]
num_retries = 8
retry_min_wait = 15
retry_max_wait = 120
retry_multiplier = 2
```
