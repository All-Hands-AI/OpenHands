# ü§ñ Backends de LLM

O OpenHands pode se conectar a qualquer LLM suportado pelo LiteLLM. No entanto, ele requer um modelo poderoso para funcionar.

## Recomenda√ß√µes de Modelo

Com base em nossas avalia√ß√µes de modelos de linguagem para tarefas de codifica√ß√£o (usando o conjunto de dados SWE-bench), podemos fornecer algumas recomenda√ß√µes para a sele√ß√£o de modelos. Nossos resultados mais recentes de benchmarking podem ser encontrados nesta [planilha](https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0).

Com base nessas descobertas e no feedback da comunidade, os seguintes modelos foram verificados como funcionando razoavelmente bem com o OpenHands:

- anthropic/claude-3-5-sonnet-20241022 (recomendado)
- anthropic/claude-3-5-haiku-20241022
- deepseek/deepseek-chat
- gpt-4o

:::warning
O OpenHands enviar√° muitos prompts para o LLM que voc√™ configurar. A maioria desses LLMs custa dinheiro, ent√£o certifique-se de definir limites de gastos e monitorar o uso.
:::

Para obter uma lista completa dos provedores e modelos dispon√≠veis, consulte a [documenta√ß√£o do litellm](https://docs.litellm.ai/docs/providers).

:::note
A maioria dos modelos locais e de c√≥digo aberto atuais n√£o s√£o t√£o poderosos. Ao usar esses modelos, voc√™ pode ver longos tempos de espera entre as mensagens, respostas ruins ou erros sobre JSON malformado. O OpenHands s√≥ pode ser t√£o poderoso quanto os modelos que o impulsionam. No entanto, se voc√™ encontrar alguns que funcionem, adicione-os √† lista verificada acima.
:::

## Configura√ß√£o do LLM

O seguinte pode ser definido na interface do usu√°rio do OpenHands por meio das Configura√ß√µes:

- `Provedor LLM`
- `Modelo LLM`
- `Chave API`
- `URL Base` (atrav√©s das configura√ß√µes `Avan√ßadas`)

Existem algumas configura√ß√µes que podem ser necess√°rias para alguns LLMs/provedores que n√£o podem ser definidas atrav√©s da interface do usu√°rio. Em vez disso, elas podem ser definidas por meio de vari√°veis de ambiente passadas para o comando docker run ao iniciar o aplicativo usando `-e`:

- `LLM_API_VERSION`
- `LLM_EMBEDDING_MODEL`
- `LLM_EMBEDDING_DEPLOYMENT_NAME`
- `LLM_DROP_PARAMS`
- `LLM_DISABLE_VISION`
- `LLM_CACHING_PROMPT`

Temos alguns guias para executar o OpenHands com provedores de modelo espec√≠ficos:

- [Azure](llms/azure-llms)
- [Google](llms/google-llms)
- [Groq](llms/groq)
- [LiteLLM Proxy](llms/litellm-proxy)
- [OpenAI](llms/openai-llms)
- [OpenRouter](llms/openrouter)

### Novas tentativas de API e limites de taxa

Os provedores de LLM normalmente t√™m limites de taxa, √†s vezes muito baixos, e podem exigir novas tentativas. O OpenHands tentar√° automaticamente as solicita√ß√µes novamente se receber um Erro de Limite de Taxa (c√≥digo de erro 429).

Voc√™ pode personalizar essas op√ß√µes conforme necess√°rio para o provedor que est√° usando. Verifique a documenta√ß√£o deles e defina as seguintes vari√°veis de ambiente para controlar o n√∫mero de novas tentativas e o tempo entre as novas tentativas:

- `LLM_NUM_RETRIES` (Padr√£o de 4 vezes)
- `LLM_RETRY_MIN_WAIT` (Padr√£o de 5 segundos)
- `LLM_RETRY_MAX_WAIT` (Padr√£o de 30 segundos)
- `LLM_RETRY_MULTIPLIER` (Padr√£o de 2)

Se voc√™ estiver executando o OpenHands no modo de desenvolvimento, tamb√©m poder√° definir essas op√ß√µes no arquivo `config.toml`:

```toml
[llm]
num_retries = 4
retry_min_wait = 5
retry_max_wait = 30
retry_multiplier = 2
```
