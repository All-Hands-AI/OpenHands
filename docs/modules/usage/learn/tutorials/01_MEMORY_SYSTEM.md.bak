# OpenHands Memory System and Knowledge Extraction Tutorial

This tutorial explains how OpenHands actually implements its memory system and knowledge extraction capabilities, based on the real codebase.

## Memory System Overview

OpenHands implements a sophisticated memory and knowledge extraction system through three main components:

1. **Event Stream** (`openhands/events/stream.py`)
   - Handles real-time event processing
   - Maintains conversation history

2. **Condenser System** (`openhands/memory/condenser/`)
   - Extracts and summarizes knowledge from events
   - Multiple condensation strategies
   - LLM-based summarization

3. **Long-Term Memory** (`openhands/memory/memory.py`)
   - Vector-based knowledge storage
   - Cross-session persistence
   - Knowledge retrieval

## Knowledge Extraction System

The key to OpenHands' knowledge extraction is its Condenser system. Let's look at how it works:

### 1. Base Condenser System
```python
# File: openhands/memory/condenser/condenser.py

class Condenser(ABC):
    """Base condenser for knowledge extraction"""
    
    def __init__(self):
        self._metadata_batch: dict[str, Any] = {}
        
    @abstractmethod
    def condense(self, events: list[Event]) -> list[Event]:
        """Condense events into knowledge"""
        pass
        
    def condensed_history(self, state: State) -> list[Event]:
        """Process state history"""
        with self.metadata_batch(state):
            return self.condense(state.history)
            
    def add_metadata(self, key: str, value: Any):
        """Track condensation metadata"""
        self._metadata_batch[key] = value
```

### 2. LLM-Based Knowledge Extraction
```python
# File: openhands/memory/condenser/impl/llm_summarizing_condenser.py

class LLMSummarizingCondenser(Condenser):
    """Extracts knowledge using LLM summarization"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        super().__init__()
        
    def condense(self, events: list[Event]) -> list[Event]:
        # Convert events to text format
        events_text = '\n'.join(
            f'{e.timestamp}: {e.message}'
            for e in events
        )
        
        # Generate knowledge summary
        resp = self.llm.completion(
            messages=[{
                'content': f'Please summarize these events:\n{events_text}',
                'role': 'user'
            }]
        )
        
        # Create condensed knowledge event
        summary = resp.choices[0].message.content
        summary_event = AgentCondensationObservation(summary)
        
        # Track metrics
        self.add_metadata('response', resp.model_dump())
        self.add_metadata('metrics', self.llm.metrics.get())
        
        return [summary_event]
```

### 3. Rolling Knowledge Accumulation
```python
# File: openhands/memory/condenser/condenser.py

class RollingCondenser(Condenser, ABC):
    """Accumulates knowledge over time"""
    
    def __init__(self):
        self._condensation: list[Event] = []
        self._last_history_length: int = 0
        super().__init__()
        
    def condensed_history(self, state: State) -> list[Event]:
        # Get new events since last condensation
        new_events = state.history[self._last_history_length:]
        
        # Condense with previous knowledge
        with self.metadata_batch(state):
            results = self.condense(
                self._condensation + new_events
            )
        
        # Update condensation state
        self._condensation = results
        self._last_history_length = len(state.history)
        
        return results
```

## Knowledge Storage and Retrieval

### 1. Vector-Based Storage
```python
# File: openhands/memory/memory.py

class LongTermMemory:
    """Stores extracted knowledge"""
    
    def __init__(self, llm_config: LLMConfig, agent_config: AgentConfig, event_stream: EventStream):
        # Initialize vector store
        self.db = chromadb.PersistentClient(
            path=f'./cache/sessions/{event_stream.sid}/memory'
        )
        self.collection = self.db.get_or_create_collection('memories')
        
        # Setup embeddings
        self.embed_model = EmbeddingsLoader.get_embedding_model(
            llm_config.embedding_model,
            llm_config
        )
        
    def add_event(self, event: Event):
        """Store event knowledge"""
        # Convert to storable format
        event_data = event_to_memory(event, -1)
        
        # Create document
        doc = Document(
            text=json.dumps(event_data),
            doc_id=str(self.thought_idx),
            extra_info={
                'type': event_type,
                'id': event_id,
                'idx': self.thought_idx
            }
        )
        
        # Store with vector embedding
        self._add_document(doc)
```

## Knowledge Flow in OpenHands

1. **Event Processing**
   ```plaintext
   User Input/System Event
          ↓
   Event Stream
          ↓
   Condenser System (Knowledge Extraction)
          ↓
   Long-Term Memory (Storage)
   ```

2. **Knowledge Retrieval**
   ```plaintext
   Query/Context
          ↓
   Vector Search
          ↓
   Relevant Knowledge
          ↓
   Agent Processing
   ```

3. **Knowledge Accumulation**
   ```plaintext
   New Events
          ↓
   Rolling Condenser
          ↓
   Updated Knowledge Summary
          ↓
   Vector Storage Update
   ```

## Using the Knowledge System

### 1. Setting Up Condensers
```python
# Configure condenser
config = LLMSummarizingCondenserConfig(
    llm_config=llm_config
)

# Create condenser
condenser = LLMSummarizingCondenser.from_config(config)
```

### 2. Processing Events
```python
# In your agent
class YourAgent(Agent):
    async def step(self, state: State) -> Action:
        # Get condensed knowledge
        condensed = self.condenser.condensed_history(state)
        
        # Use in context
        context = {
            'history': condensed,
            'current_state': state
        }
        
        # Generate response
        response = await self.llm.generate(
            prompt=self._create_prompt(state),
            context=context
        )
        
        return self._create_action(response)
```

### 3. Accessing Stored Knowledge
```python
# Search knowledge
results = memory.search(
    query="relevant context",
    k=10  # number of results
)

# Use in processing
context = {
    'knowledge': results,
    'current_input': current_event
}
```

## Best Practices

1. **Knowledge Extraction**
   - Use appropriate condenser for your needs
   - Monitor condensation quality
   - Track metadata for debugging

2. **Storage Management**
   - Regular cleanup of old sessions
   - Monitor vector store size
   - Validate stored knowledge

3. **Performance**
   - Use batch processing when possible
   - Cache frequent queries
   - Monitor embedding performance

4. **Knowledge Quality**
   - Validate extracted knowledge
   - Update outdated information
   - Track knowledge confidence
OpenHands Memory Architecture
│
├── Short-Term Memory (Event History)
│   ├── Implementation: EventStream
│   ├── Location: openhands/events/stream.py
│   └── Purpose: Maintain current conversation context
│
├── Memory Condenser
│   ├── Implementation: Various Condensers
│   ├── Location: openhands/memory/condenser/
│   └── Purpose: Summarize and compress history
│
└── Long-Term Memory
    ├── Implementation: ChromaDB + Vector Store
    ├── Location: openhands/memory/memory.py
    └── Purpose: Persistent knowledge storage
```

## 1. Short-Term Memory (Event Stream)

```python
# File: openhands/events/stream.py

class EventStream:
    """Manages current conversation context"""
    
    def __init__(self, sid: str, file_store: FileStore):
        self.sid = sid
        self.file_store = file_store
        self.subscribers = {}
        self.history = []
        
    async def emit(self, event: Event):
        """Add event to history and notify subscribers"""
        # Store event
        self.history.append(event)
        
        # Notify subscribers
        for subscriber in self.subscribers:
            try:
                await subscriber.handle_event(event)
            except Exception as e:
                logger.error(f"Subscriber error: {e}")
```

## 2. Memory Condenser System

OpenHands provides multiple condensation strategies:

### LLM Summarizing Condenser
```python
# File: openhands/memory/condenser/impl/llm_summarizing_condenser.py

class LLMSummarizingCondenser(Condenser):
    """Uses LLM to summarize event sequences"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        super().__init__()
        
    def condense(self, events: list[Event]) -> list[Event]:
        # Convert events to text
        events_text = '\n'.join(
            f'{e.timestamp}: {e.message}'
            for e in events
        )
        
        # Generate summary
        resp = self.llm.completion(
            messages=[{
                'content': f'Please summarize these events:\n{events_text}',
                'role': 'user'
            }]
        )
        
        # Create summary event
        return [AgentCondensationObservation(
            resp.choices[0].message.content
        )]
```

### Other Condensers
- AmortizedForgettingCondenser: Gradually forgets old events
- LLMAttentionCondenser: Uses attention mechanisms
- ObservationMaskingCondenser: Masks irrelevant details

## 3. Long-Term Memory System

```python
# File: openhands/memory/memory.py

class LongTermMemory:
    """Persistent knowledge storage using vector database"""
    
    def __init__(
        self,
        llm_config: LLMConfig,
        agent_config: AgentConfig,
        event_stream: EventStream
    ):
        # Initialize ChromaDB
        self.db = chromadb.PersistentClient(
            path=f'./cache/sessions/{event_stream.sid}/memory'
        )
        self.collection = self.db.get_or_create_collection(
            name='memories'
        )
        
        # Setup vector store
        self.vector_store = ChromaVectorStore(
            chroma_collection=self.collection
        )
        
        # Initialize embedding model
        self.embed_model = EmbeddingsLoader.get_embedding_model(
            llm_config.embedding_model,
            llm_config
        )
        
    def add_event(self, event: Event):
        """Store event in long-term memory"""
        # Convert event to storable format
        event_data = event_to_memory(event, -1)
        
        # Create document
        doc = Document(
            text=json.dumps(event_data),
            doc_id=str(self.thought_idx),
            extra_info={
                'type': event_type,
                'id': event_id,
                'idx': self.thought_idx
            }
        )
        
        # Store in vector database
        self._add_document(doc)
        
    def search(self, query: str, k: int = 10) -> list[str]:
        """Search through memory"""
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=k
        )
        return [r.get_text() for r in retriever.retrieve(query)]
```

## Cross-Session Knowledge Sharing

OpenHands implements cross-session knowledge sharing through:

1. **Persistent Storage**
```python
# Knowledge is stored in ChromaDB at:
f'./cache/sessions/{session_id}/memory'

# This allows:
- Session-specific storage
- Persistent across restarts
- Vector-based retrieval
```

2. **Knowledge Retrieval**
```python
class LongTermMemory:
    def search(self, query: str, k: int = 10) -> list[str]:
        """Retrieve relevant knowledge"""
        # Create retriever
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=k
        )
        
        # Search for similar content
        results = retriever.retrieve(query)
        
        # Return matched documents
        return [r.get_text() for r in results]
```

3. **Event Processing**
```python
class EventStream:
    async def process_event(self, event: Event):
        """Process and store events"""
        # Add to history
        self.history.append(event)
        
        # Store in long-term memory
        if self.memory:
            await self.memory.add_event(event)
            
        # Notify subscribers
        await self._notify_subscribers(event)
```

## Using the Memory System

### 1. Accessing Previous Knowledge
```python
# In your agent implementation
class YourAgent(Agent):
    async def step(self, state: State) -> Action:
        # Get current input
        current_input = state.get_latest_input()
        
        # Search relevant knowledge
        relevant_knowledge = await self.memory.search(
            current_input.message
        )
        
        # Use knowledge in context
        context = {
            'history': state.history,
            'knowledge': relevant_knowledge
        }
        
        # Generate response
        response = await self.llm.generate(
            prompt=self._create_prompt(current_input),
            context=context
        )
        
        return self._create_action(response)
```

### 2. Storing New Knowledge
```python
# When processing events
async def process_event(event: Event):
    # Store in memory
    await memory.add_event(event)
    
    # Process event
    if is_learning_opportunity(event):
        # Extract knowledge
        knowledge = extract_knowledge(event)
        
        # Store as document
        doc = create_document(knowledge)
        await memory._add_document(doc)
```

### 3. Managing Memory
```python
# Regular cleanup
async def cleanup_memory():
    # Find old sessions
    old_sessions = find_old_sessions()
    
    for session in old_sessions:
        # Archive important knowledge
        knowledge = extract_important_knowledge(session)
        await archive_knowledge(knowledge)
        
        # Clean up session
        await cleanup_session(session)
```

## Best Practices

1. **Memory Usage**
   - Monitor memory size
   - Clean up old sessions
   - Archive important knowledge

2. **Knowledge Quality**
   - Validate before storage
   - Remove duplicates
   - Update outdated information

3. **Performance**
   - Use appropriate batch sizes
   - Index important fields
   - Cache frequent queries

4. **Privacy**
   - Don't store sensitive data
   - Implement access controls
   - Allow data deletion

## Current Limitations

1. **Storage**
   - Session-based storage only
   - No cross-user sharing
   - Limited categorization

2. **Retrieval**
   - Basic similarity search
   - No complex querying
   - Limited context understanding

3. **Learning**
   - No active learning
   - Basic pattern recognition
   - Limited preference tracking

## Future Improvements

1. **Enhanced Storage**
   - Global knowledge base
   - Better categorization
   - Improved metadata

2. **Smart Retrieval**
   - Context-aware search
   - Multi-modal queries
   - Relevance ranking

3. **Active Learning**
   - Continuous improvement
   - Pattern learning
   - Preference adaptation