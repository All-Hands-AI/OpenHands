---
title: Backend Architecture
---


This is a high-level overview of the system architecture. The system is divided into two main components: the frontend and the backend. The frontend is responsible for handling user interactions and displaying the results. The backend is responsible for handling the business logic and executing the agents.

# System overview

```mermaid
flowchart LR
  U["User"] --> FE["Frontend (SPA)"]
  FE -- "HTTP/WS" --> BE["OpenHands Backend"]
  BE --> ES["EventStream"]
  BE --> ST["Storage"]
  BE --> RT["Runtime Interface"]
  BE --> LLM["LLM Providers"]

  subgraph Runtime
    direction TB
    RT --> DRT["Docker Runtime"]
    RT --> LRT["Local Runtime"]
    RT --> RRT["Remote Runtime"]
    DRT --> AES["Action Execution Server"]
    LRT --> AES
    RRT --> AES
    AES --> Bash["Bash Session"]
    AES --> Jupyter["Jupyter Plugin"]
    AES --> Browser["BrowserEnv"]
  end
```

This Overview is simplified to show the main components and their interactions. For a more detailed view of the backend architecture, see the Backend Architecture section below.

# Backend Architecture


```mermaid
classDiagram
  class Agent {
    <<abstract>>
    +sandbox_plugins: list[PluginRequirement]
  }
  class CodeActAgent {
    +tools
  }
  Agent <|-- CodeActAgent

  class EventStream
  class Observation
  class Action
  Action --> Observation
  Agent --> EventStream

  class Runtime {
    +connect()
    +send_action_for_execution()
  }
  class ActionExecutionClient {
    +_send_action_server_request()
  }
  class DockerRuntime
  class LocalRuntime
  class RemoteRuntime
  Runtime <|-- ActionExecutionClient
  ActionExecutionClient <|-- DockerRuntime
  ActionExecutionClient <|-- LocalRuntime
  ActionExecutionClient <|-- RemoteRuntime

  class ActionExecutionServer {
    +/execute_action
    +/alive
  }
  class BashSession
  class JupyterPlugin
  class BrowserEnv
  ActionExecutionServer --> BashSession
  ActionExecutionServer --> JupyterPlugin
  ActionExecutionServer --> BrowserEnv

  Agent --> Runtime
  Runtime ..> ActionExecutionServer : REST
```

<details>
  <summary>Updating this Diagram</summary>
  <div>
    We maintain architecture diagrams inline with Mermaid in this MDX.

    Guidance:
    - Edit the Mermaid blocks directly (flowchart/classDiagram).
    - Quote labels and edge text for GitHub preview compatibility.
    - Keep relationships concise and reflect stable abstractions (agents, runtime client/server, plugins).
    - Verify accuracy against code:
      - openhands/runtime/impl/action_execution/action_execution_client.py
      - openhands/runtime/impl/docker/docker_runtime.py
      - openhands/runtime/impl/local/local_runtime.py
      - openhands/runtime/action_execution_server.py
      - openhands/runtime/plugins/*
    - Build docs locally or view on GitHub to confirm diagrams render.

  </div>
</details>

## LLM architecture: LLMRegistry and ConversationStats

The LLMRegistry centralizes creation, caching, and configuration of all LLM instances across the backend. Components request LLMs by service_id (e.g., "agent", "condenser", "guardrail") rather than directly constructing them. This enables per-service configuration, unified retries, and consistent accounting via ConversationStats.

Key points:
- **Single source of truth**: LLMRegistry manages all LLM instances per conversation/session
- **Service-scoped instances**: Components request LLMs by service_id, no direct construction
- **Unified retry handling**: Registry attaches retry listeners for consistent error handling
- **Usage tracking**: ConversationStats subscribes to registry events for accounting
- **Extraneous completions**: One-off utility calls (e.g., title generation) go through registry

### 1. System initialization and wiring

```mermaid
flowchart TD
  A["1. Session/CLI start"] --> B["2. create_registry_and_convo_stats()"]
  B --> C["3. LLMRegistry created"]
  B --> D["4. ConversationStats created"]
  
  C -->|"5. subscribe()"| D
  D -.->|"6. register_llm callback"| C
  
  C --> E["7. Agent constructed with registry"]
  C --> F["8. Runtime constructed with registry"]
  D --> G["9. AgentController constructed with stats"]
  
  H["10. Session sets retry listener"] -.->|"config"| C
  
  style A fill:#e1f5fe
  style B fill:#e8f5e8
  style C fill:#fff3e0
  style D fill:#fce4ec
  style E fill:#f3e5f5
  style F fill:#f3e5f5
  style G fill:#fce4ec
  style H fill:#e1f5fe
  
  classDef initPhase fill:#e1f5fe,stroke:#01579b,stroke-width:2px
  classDef registryPhase fill:#fff3e0,stroke:#e65100,stroke-width:2px
  classDef statsPhase fill:#fce4ec,stroke:#880e4f,stroke-width:2px
  classDef componentPhase fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
```

### 2. Core classes and relationships

```mermaid
classDiagram
  direction TB

  class LLMRegistry {
    -config: OpenHandsConfig
    -retry_listener: Callable
    -service_to_llm: Map~string,LLM~
    +get_llm(service_id, config?) LLM
    +subscribe(callback) void
    +notify(event) void
  }

  class LLM {
    -service_id: string
    -config: LLMConfig
    -retry_listener: Callable
    +completion(messages, stream) Response
  }

  class ConversationStats {
    -service_to_metrics: Map~string,Metrics~
    +register_llm(event) void
    +get_combined_metrics() Metrics
  }

  class Agent {
    -llm_registry: LLMRegistry
    -llm: LLM
  }

  class Session {
    -llm_registry: LLMRegistry
    -convo_stats: ConversationStats
    +_notify_on_llm_retry(retries, max) void
  }

  %% Creation relationships
  LLMRegistry -->|"creates & caches"| LLM
  
  %% Usage relationships  
  Agent -->|"requests LLM by service_id"| LLMRegistry
  Agent -->|"uses for completions"| LLM
  
  %% Configuration relationships
  Session -.->|"sets retry_listener"| LLMRegistry
  
  %% Event flow relationships
  LLM -->|"calls on retry"| Session
  LLMRegistry -->|"notifies on LLM creation"| ConversationStats
  Session -->|"updates on retry/tokens"| ConversationStats
```

### 3. Runtime flow: LLM request and completion

```mermaid
sequenceDiagram
  participant Service as Component<br/>(Agent/Condenser/etc)
  participant Registry as LLMRegistry
  participant LLM as LLM Instance
  participant Session as Session
  participant Stats as ConversationStats
  participant UI as UI

  Note over Service,UI: Normal LLM Request Flow
  
  Service->>+Registry: 1. get_llm(service_id, config?)
  
  alt LLM exists in cache
    Registry-->>Service: return cached LLM
  else LLM needs creation
    Registry->>+LLM: 2. create LLM with retry_listener
    Registry->>Stats: 3. notify(RegistryEvent)
    Stats->>Stats: 4. register_llm() - track metrics
    Registry-->>-Service: 5. return new LLM
  end
  
  Service->>+LLM: 6. completion(messages, stream?)
  
  loop on transient errors (rate limits, timeouts, etc)
    LLM->>Session: 7. retry_listener(retries, max)
    Session->>Stats: 8. record retry event  
    Session->>UI: 9. queue_status_message("llm-retry")
    Note over LLM: Wait and retry with backoff
  end
  
  LLM-->>-Service: 10. completion response
  Note over Stats: Metrics automatically updated<br/>via LLM.metrics reference
```

### 4. ConversationStats lifecycle

```mermaid
flowchart TD
  A["Session start"] --> B["ConversationStats created"]
  B --> C["Subscribe to LLMRegistry events"]
  
  subgraph "Runtime Operations"
    D["Component requests LLM"] --> E["LLMRegistry creates/returns LLM"]
    E --> F["ConversationStats.register_llm() called"]
    F --> G["Track LLM metrics reference"]
    
    H["LLM completion occurs"] --> I["Metrics updated automatically"]
    I --> J["Retry events recorded via Session"]
  end
  
  C --> D
  
  K["Session end"] --> L["ConversationStats.save_metrics()"]
  L --> M["Persist to FileStore"]
  
  style A fill:#e1f5fe
  style B fill:#fce4ec
  style C fill:#fce4ec
  style K fill:#e1f5fe
  style L fill:#fce4ec
  style M fill:#e8f5e8
```

### Practical usage examples

- Agent resolution (base Agent resolves the LLM via the registry):

```python
agent = CodeActAgent(config=agent_config, llm_registry=llm_registry)
# self.llm is automatically resolved from config via the registry
```

- LLM-backed condenser from config:

```python
condenser = Condenser.from_config(config.condenser, llm_registry)
```

- One-off utility call (e.g., conversation title generation):

```python
title = llm_registry.request_extraneous_completion(
    'conversation_title',
    messages=[{'role': 'user', 'content': first_user_message}],
)
```

