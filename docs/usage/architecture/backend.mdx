---
title: Backend Architecture
---


This is a high-level overview of the system architecture. The system is divided into two main components: the frontend and the backend. The frontend is responsible for handling user interactions and displaying the results. The backend is responsible for handling the business logic and executing the agents.

# System overview

```mermaid
flowchart LR
  U["User"] --> FE["Frontend (SPA)"]
  FE -- "HTTP/WS" --> BE["OpenHands Backend"]
  BE --> ES["EventStream"]
  BE --> ST["Storage"]
  BE --> RT["Runtime Interface"]
  BE --> LLM["LLM Providers"]

  subgraph Runtime
    direction TB
    RT --> DRT["Docker Runtime"]
    RT --> LRT["Local Runtime"]
    RT --> RRT["Remote Runtime"]
    DRT --> AES["Action Execution Server"]
    LRT --> AES
    RRT --> AES
    AES --> Bash["Bash Session"]
    AES --> Jupyter["Jupyter Plugin"]
    AES --> Browser["BrowserEnv"]
  end
```

This Overview is simplified to show the main components and their interactions. For a more detailed view of the backend architecture, see the Backend Architecture section below.

# Backend Architecture


```mermaid
classDiagram
  class Agent {
    <<abstract>>
    +sandbox_plugins: list[PluginRequirement]
  }
  class CodeActAgent {
    +tools
  }
  Agent <|-- CodeActAgent

  class EventStream
  class Observation
  class Action
  Action --> Observation
  Agent --> EventStream

  class Runtime {
    +connect()
    +send_action_for_execution()
  }
  class ActionExecutionClient {
    +_send_action_server_request()
  }
  class DockerRuntime
  class LocalRuntime
  class RemoteRuntime
  Runtime <|-- ActionExecutionClient
  ActionExecutionClient <|-- DockerRuntime
  ActionExecutionClient <|-- LocalRuntime
  ActionExecutionClient <|-- RemoteRuntime

  class ActionExecutionServer {
    +/execute_action
    +/alive
  }
  class BashSession
  class JupyterPlugin
  class BrowserEnv
  ActionExecutionServer --> BashSession
  ActionExecutionServer --> JupyterPlugin
  ActionExecutionServer --> BrowserEnv

  Agent --> Runtime
  Runtime ..> ActionExecutionServer : REST
```

<details>
  <summary>Updating this Diagram</summary>
  <div>
    We maintain architecture diagrams inline with Mermaid in this MDX.

    Guidance:
    - Edit the Mermaid blocks directly (flowchart/classDiagram).
    - Quote labels and edge text for GitHub preview compatibility.
    - Keep relationships concise and reflect stable abstractions (agents, runtime client/server, plugins).
    - Verify accuracy against code:
      - openhands/runtime/impl/action_execution/action_execution_client.py
      - openhands/runtime/impl/docker/docker_runtime.py
      - openhands/runtime/impl/local/local_runtime.py
      - openhands/runtime/action_execution_server.py
      - openhands/runtime/plugins/*
    - Build docs locally or view on GitHub to confirm diagrams render.

  </div>
</details>

## LLM architecture: LLMRegistry and ConversationStats

The LLMRegistry centralizes creation, caching, and configuration of all LLM instances across the backend. Components request LLMs by service_id (e.g., "agent", "condenser", "guardrail") rather than directly constructing them. This enables per-service configuration, unified retries, and consistent accounting via ConversationStats.

Key points:
- Single source of truth for LLMs per conversation/session
- Instances keyed by service_id; components do not new-up LLMs directly
- Registry attaches a retry listener so UI/Session can surface transient issues
- One-off utility completions go through the registry (extraneous requests)
- Conversation-wide usage and status reporting flows through ConversationStats

### Core classes and relationships

```mermaid
classDiagram
  direction LR

  class OpenHandsConfig {
    +get_llm_config(): LLMConfig
    +get_llm_config_from_agent(agentId): LLMConfig
  }

  class LLMConfig {
    model
    api_key
    base_url
    ...
  }

  class LLMRegistry {
    -config: OpenHandsConfig
    -retry_listener: Callable
    -cache: Map<string, LLM>
    +get_llm(service_id, config?): LLM
    +get_llm_from_agent_config(service_id, AgentConfig): LLM
    +request_extraneous_completion(service_id, ...): string
  }

  class LLM {
    -service_id: string
    -config: LLMConfig
    -retry_listener: Callable
    +completion(messages, stream)
  }

  class Agent {
    -config: AgentConfig
    -llm_registry: LLMRegistry
    -llm: LLM
  }

  class LLMSummarizingCondenser { -llm: LLM }
  class Runtime { -llm_registry: LLMRegistry }
  class AgentController { -convo_stats: ConversationStats }
  class ConversationStats { +record(tokens, cost, retries) }
  class Session { +_notify_on_llm_retry(...) }

  LLMRegistry --> LLM : creates/caches
  Agent --> LLMRegistry : uses
  Agent --> LLM : resolves ("agent")
  LLMSummarizingCondenser --> LLMRegistry : uses
  Runtime --> LLMRegistry : holds
  AgentController --> ConversationStats : uses
  LLM --> Session : retry_listener()
  Session ..> LLMRegistry : sets registry.retry_listener
  Session --> ConversationStats : update on retries/tokens
```

### When a component requests an LLM and performs a completion

```mermaid
sequenceDiagram
  participant Service as Component/Service (service_id)
  participant Registry as LLMRegistry
  participant Config as OpenHandsConfig
  participant LLM as LLM
  participant Session as Session
  participant Stats as ConversationStats

  Service->>Registry: get_llm(service_id, [config])
  alt config omitted
    Registry->>Config: resolve LLMConfig (agent/global)
  end
  Registry->>LLM: construct if needed (attach retry_listener)
  Registry-->>Service: return LLM instance

  Service->>LLM: completion(messages, stream?)
  loop on transient errors
    LLM-->>Session: retry_listener(retry_ctx)
    Session->>Stats: record retry event
    Session->>UI: queue_status_message(type="llm-retry", details)
  end
  LLM-->>Service: response
  Note over Registry,Stats: "Centralized accounting (tokens/cost) per service_id"
```

### ConversationStats lifecycle

```mermaid
flowchart TD
  A[Session or CLI start] --> B[create registry and convo stats]
  B -->|LLMRegistry| C[LLM Registry]
  B -->|ConversationStats| D[Conversation Stats]
  C --> E[Agent constructed]
  C --> G[Runtime constructed]
  D --> H[AgentController constructed with convo stats]
  I[Session] --> J[Set retry listener on LLM Registry]

  subgraph During_Conversation
    K[Any component requests LLM] --> C
    C --> L[LLM instance service id scoped]
    L --> M[completion]
    M -->|success| N[Conversation Stats add tokens cost]
    M -->|retry| I
    I --> D[Conversation Stats record retry and notify UI]
  end

  O[End and teardown] --> P[Conversation Stats finalize persist via FileStore]
  D --> P
```

### Practical usage examples

- Agent resolution (base Agent resolves the LLM via the registry):

```python
agent = CodeActAgent(config=agent_config, llm_registry=llm_registry)
# self.llm is automatically resolved from config via the registry
```

- LLM-backed condenser from config:

```python
condenser = Condenser.from_config(config.condenser, llm_registry)
```

- One-off utility call (e.g., conversation title generation):

```python
title = llm_registry.request_extraneous_completion(
    'conversation_title',
    messages=[{'role': 'user', 'content': first_user_message}],
)
```

